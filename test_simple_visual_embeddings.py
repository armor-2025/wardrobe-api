"""
Simpler approach - use sentence-transformers CLIP model
"""
import os
import sys

print("=" * 70)
print("ğŸ”§ ALTERNATIVE: Using sentence-transformers")
print("=" * 70)

print("\nğŸ“¦ Installing sentence-transformers (lighter than transformers)...")
os.system("pip install sentence-transformers pillow scipy --break-system-packages")

print("\nâœ… Testing...")

from sentence_transformers import SentenceTransformer
from PIL import Image

print("\nğŸ“¥ Loading CLIP model...")
model = SentenceTransformer('clip-ViT-B-32')
print("âœ… Model loaded!")

# Test with an image
if len(sys.argv) > 1:
    image_path = sys.argv[1]
else:
    image_path = os.path.expanduser('~/Desktop/AI OUTFIT PICS/IMG_6561.PNG')

print(f"\nğŸ“¸ Testing with: {image_path}")

image = Image.open(image_path)

print("\nğŸ”„ Generating visual embedding...")
embedding = model.encode(image)

print("âœ… Embedding generated!")
print(f"   Dimensions: {len(embedding)}")
print(f"   First 10 values: {[f'{x:.4f}' for x in embedding[:10]]}")

print("\n" + "=" * 70)
print("ğŸ¯ TESTING WITH MULTIPLE IMAGES")
print("=" * 70)

# Test similarity
test_images = [
    '~/Desktop/AI OUTFIT PICS/IMG_6561.PNG',
    '~/Desktop/AI OUTFIT PICS/IMG_6563.PNG',
    '~/Desktop/AI OUTFIT PICS/IMG_6565.PNG',
]

embeddings = []
print("\nGenerating embeddings for comparison...")

for img_path in test_images:
    full_path = os.path.expanduser(img_path)
    if os.path.exists(full_path):
        img = Image.open(full_path)
        emb = model.encode(img)
        embeddings.append((full_path.split('/')[-1], emb))
        print(f"  âœ… {full_path.split('/')[-1]}")

if len(embeddings) >= 2:
    print("\nğŸ“Š Similarity Scores:")
    print("â”€" * 70)
    
    from scipy.spatial.distance import cosine
    
    base_name, base_emb = embeddings[0]
    
    for name, emb in embeddings[1:]:
        similarity = 1 - cosine(base_emb, emb)
        print(f"  {base_name} <-> {name}")
        print(f"  Similarity: {similarity:.2%}\n")

print("\n" + "=" * 70)
print("âœ… SUCCESS - VISUAL EMBEDDINGS WORKING!")
print("=" * 70)

print("""
This model:
âœ… Generates 512-dimensional embeddings
âœ… Works with images directly
âœ… Perfect for visual similarity search
âœ… Production-ready

WHAT THIS MEANS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
We can now find products that LOOK similar!

Example:
  User uploads: Green blazer photo
  We generate: embedding = [0.23, 0.45, 0.12, ...]
  
  Database has:
    Product A (green blazer): [0.24, 0.44, 0.13, ...] â†’ 98% similar âœ…
    Product B (red dress):    [0.91, 0.05, 0.88, ...] â†’ 25% similar âŒ
  
  Return Product A!

ğŸ’° COST: $0.00 per embedding (runs locally)

ğŸš€ NEXT STEPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Set up Supabase database
2. Import products with embeddings
3. Build search API
4. Ship it!

Ready to continue?
""")

